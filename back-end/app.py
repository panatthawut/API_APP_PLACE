from fastapi import FastAPI, File, UploadFile, HTTPException, Request,Form
from fastapi.middleware.cors import CORSMiddleware
import joblib
import numpy as np
from tensorflow.keras.applications.efficientnet import EfficientNetB7, preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import GlobalAveragePooling2D
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from PIL import Image
import io
import logging
import os
from sklearn.neighbors import NearestNeighbors
from pydantic import BaseModel
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score, train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
logging.basicConfig(level=logging.INFO)

app = FastAPI()

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î CORS ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API ‡∏à‡∏≤‡∏Å‡πÅ‡∏≠‡∏û Flutter ‡πÑ‡∏î‡πâ
origins = [
    "http://localhost",
    "http://localhost:8000",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Path configurations
FEATURES_PATH = "models/features.npy"
MODEL_PATH = "models/knn_model.joblib"
TRAINED_MODEL_PATH = 'models/efficientnetb7_weights.weights.h5'
LABELS_PATH = "models/labels.npy"
INDEX_TO_CLASS_PATH = "models/index_to_class.joblib"
TRAIN_INDICES_TO_CLASS = "models/train_indices_to_class.joblib"
BASE_PATH = "C:/xampp/htdocs/api_php/"
FEATURES_NORMALIZED_PATH ="models/features_normalized.npy"
# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ mapping
knn_model = NearestNeighbors(n_neighbors=5)
try:
    if os.path.exists(MODEL_PATH):
        knn_model = joblib.load(MODEL_PATH)
        logging.info("KNN model loaded successfully.")
except Exception as e:
    logging.error(f"Error loading KNN model: {e}")

index_to_class = {}
try:
    index_to_class = joblib.load('models/index_to_class.joblib')
    logging.info("Index to class mapping loaded successfully.")
except Exception as e:
    logging.error(f"Error loading index_to_class: {e}")


train_indices_to_class = {}
try:
    train_indices_to_class = joblib.load('models/train_indices_to_class.joblib')
    logging.info("Train indices to class mapping loaded successfully.")
except Exception as e:
    logging.error(f"Error loading train_indices_to_class: {e}")
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á EfficientNetB7
base_model = EfficientNetB7(weights=None, include_top=False, input_shape=(600, 600, 3))
x = base_model.output
x = GlobalAveragePooling2D()(x)
efficientnet_model = Model(inputs=base_model.input, outputs=x)

# ‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
try:
    if os.path.exists(TRAINED_MODEL_PATH):
        efficientnet_model.load_weights(TRAINED_MODEL_PATH)
        logging.info("EfficientNet weights loaded successfully.")
except Exception as e:
    logging.error(f"Error loading EfficientNet weights: {e}")
def load_models():
    knn_model = NearestNeighbors(n_neighbors=5)
    try:
        if os.path.exists(MODEL_PATH):
            knn_model = joblib.load(MODEL_PATH)
            logging.info("KNN model loaded successfully.")
    except Exception as e:
        logging.error(f"Error loading KNN model: {e}")

    index_to_class = {}
    try:
        index_to_class = joblib.load('models/index_to_class.joblib')
        logging.info("Index to class mapping loaded successfully.")
    except Exception as e:
        logging.error(f"Error loading index_to_class: {e}")


    train_indices_to_class = {}
    try:
        train_indices_to_class = joblib.load('models/train_indices_to_class.joblib')
        logging.info("Train indices to class mapping loaded successfully.")
    except Exception as e:
        logging.error(f"Error loading train_indices_to_class: {e}")

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á EfficientNetB7
    base_model = EfficientNetB7(weights=None, include_top=False, input_shape=(600, 600, 3))
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    efficientnet_model = Model(inputs=base_model.input, outputs=x)

    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
    try:
        if os.path.exists(TRAINED_MODEL_PATH):
            efficientnet_model.load_weights(TRAINED_MODEL_PATH)
            logging.info("EfficientNet weights loaded successfully.")
    except Exception as e:
        logging.error(f"Error loading EfficientNet weights: {e}")
load_models()
labels = np.load(LABELS_PATH)
features = np.load(FEATURES_PATH)
print('labels',labels)
print('index_to_class',index_to_class)
print('knn_model',knn_model)
print('features',features)
#print('train_indices_to_class',train_indices_to_class)
index_to_class = joblib.load(INDEX_TO_CLASS_PATH)
features = np.load(FEATURES_PATH)
logging.info(f"Loaded features with shape: {features.shape}")
logging.info(f"Loaded labels with shape: {labels.shape}")
def preprocess_image_from_bytes(image_bytes):
    img = Image.open(io.BytesIO(image_bytes)).convert('RGB')
    img = img.resize((600, 600))  # ‡∏Ç‡∏ô‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    img_array = np.array(img)
    img_array = preprocess_input(img_array)  # ‡πÉ‡∏ä‡πâ preprocessing ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô
    img_array = np.expand_dims(img_array, axis=0)
    return img_array
import requests
from math import radians, sin, cos, sqrt, atan2
from math import log
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Haversine ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á
def haversine(lat1, lon1, lat2, lon2):
    try:
        R = 6371.0  # ‡∏£‡∏±‡∏®‡∏°‡∏µ‡∏Ç‡∏≠‡∏á‡πÇ‡∏•‡∏Å (‡∏Å‡∏¥‡πÇ‡∏•‡πÄ‡∏°‡∏ï‡∏£)
        dlat = radians(lat2 - lat1)
        dlon = radians(lon2 - lon1)
        a = sin(dlat / 2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon / 2)**2
        c = 2 * atan2(sqrt(a), sqrt(1 - a))
        return R * c  # ‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏¥‡πÇ‡∏•‡πÄ‡∏°‡∏ï‡∏£
    except Exception as e:
        logging.error(f"Error in haversine calculation: {e}")
        return float("inf")  # ‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏î‡πâ

@app.post("/predict_gallery")
async def predict(
    image: UploadFile = File(...),
    latitude: float = Form(...),
    longitude: float = Form(...),
):
    logging.info(f"Received latitude: {latitude}, longitude: {longitude}")

    if not image:
        raise HTTPException(status_code=400, detail="No image provided")

    try:
        image_bytes = await image.read()
        processed_image = preprocess_image_from_bytes(image_bytes)
        logging.info("Image preprocessing completed.")

        # ‡∏™‡∏Å‡∏±‡∏î‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏à‡∏≤‡∏Å EfficientNet
        features = efficientnet_model.predict(processed_image)
        features = features.flatten()
        logging.info(f"Extracted features shape: {features.shape}")

        # ‡πÉ‡∏ä‡πâ KNN ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ 50 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏ö‡πâ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        distances, indices = knn_model.kneighbors([features], n_neighbors=50)
        logging.info(f"Distances: {distances}")
        logging.info(f"Indices: {indices}")

        nearest_locations = []
        class_similarity_map = {}

        for index in indices[0]:
            location = train_indices_to_class.get(index, "Unknown")
            similarity = round((1 - distances[0][np.where(indices[0] == index)[0][0]]) * 100, 2)

            if location not in class_similarity_map:
                class_similarity_map[location] = similarity
            else:
                class_similarity_map[location] = max(class_similarity_map[location], similarity)

        sorted_locations = sorted(class_similarity_map.items(), key=lambda x: x[1], reverse=True)

        for location, similarity in sorted_locations:
            if location not in [loc["location"] for loc in nearest_locations]:
                nearest_locations.append({"location": location, "similarity": similarity})
            if len(nearest_locations) == 5:
                break
        logging.info(f"All locations with scores: {class_similarity_map}")

        logging.info(f"Nearest Locations: {nearest_locations}")

        return {"nearest_locations": nearest_locations}
    except Exception as e:
        logging.error(f"Error during prediction: {e}")
        raise HTTPException(status_code=500, detail=str(e))
@app.post("/predict_cameraa")
async def predict(
    image: UploadFile = File(...),
    latitude: float = Form(...),
    longitude: float = Form(...),
):
    max_distance = 50  # ‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î (‡∏´‡∏ô‡πà‡∏ß‡∏¢: ‡∏Å‡∏¥‡πÇ‡∏•‡πÄ‡∏°‡∏ï‡∏£)
    logging.info(f"Received latitude: {latitude}, longitude: {longitude}, max_distance: {max_distance}")

    if not image:
        raise HTTPException(status_code=400, detail="No image provided")

    try:
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏à‡∏≤‡∏Å get_lat_long_location.php
        location_response = requests.get("http://localhost/api_php/get_lat_long_location.php")
        if location_response.status_code != 200:
            raise HTTPException(status_code=500, detail="Failed to fetch location data")

        location_data = location_response.json()
        logging.info(f"Fetched location data: {location_data}")

        # ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
        image_bytes = await image.read()
        processed_image = preprocess_image_from_bytes(image_bytes)
        logging.info("Image preprocessing completed.")

        # ‡∏™‡∏Å‡∏±‡∏î‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏à‡∏≤‡∏Å EfficientNet
        features = efficientnet_model.predict(processed_image)
        features = features.flatten()
        logging.info(f"Extracted features shape: {features.shape}")

        # ‡πÉ‡∏ä‡πâ KNN ‡∏´‡∏≤ 50 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏ö‡πâ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        distances, indices = knn_model.kneighbors([features], n_neighbors=50)
        logging.info(f"Distances: {distances}")
        logging.info(f"Indices: {indices}")

        nearest_locations = []
        all_nearby_locations = []  # ‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
        class_distance_map = {}

        for index in indices[0]:
            location = train_indices_to_class.get(index, "Unknown")
            similarity = round((1 - distances[0][np.where(indices[0] == index)[0][0]]) * 100, 2)

            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà
            matched_location = next(
                (loc for loc in location_data if loc["Location_Class"] == location), None
            )
            if matched_location:
                loc_lat = float(matched_location["Location_Lat"])
                loc_lon = float(matched_location["Location_Long"])
                distance = haversine(latitude, longitude, loc_lat, loc_lon)
                distance=distance*0.01
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
                if distance <= max_distance:
                    all_nearby_locations.append({
                        "location": location,
                        "distance": round(distance, 2),
                        "latitude": loc_lat,
                        "longitude": loc_lon
                    })

                    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á
                    combined_score = (0.7 * similarity) - (0.3 * distance)
                    class_distance_map[location] = {
                        "similarity": similarity,
                        "combined_score": max(
                            class_distance_map.get(location, {}).get("combined_score", float("-inf")),
                            combined_score,
                        )
                    }

        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°
        sorted_locations = sorted(
            class_distance_map.items(),
            key=lambda x: x[1]["combined_score"],
            reverse=True
        )

        for location, scores in sorted_locations:
            if location not in [loc["location"] for loc in nearest_locations]:
                nearest_locations.append({
                    "location": location,
                    "similarity": scores["similarity"],
                    "combined_score": round(scores["combined_score"], 2)
                })
            if len(nearest_locations) == 5:  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà 5 ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà
                break

        logging.info(f"Nearest Locations: {nearest_locations}")
        logging.info(f"All Nearby Locations: {all_nearby_locations}")
        return {
            "nearest_locations": nearest_locations,
            "all_nearby_locations": all_nearby_locations  # ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
        }

    except Exception as e:
        logging.error(f"Error during prediction: {e}")
        raise HTTPException(status_code=500, detail=str(e))
@app.post("/predict_camera")
async def predict(
    image: UploadFile = File(...),
    latitude: float = Form(...),
    longitude: float = Form(...),
    max_distance: float = Form(...)
):
    logging.info(f"Received latitude: {latitude}, longitude: {longitude}")
    if not image:
        raise HTTPException(status_code=400, detail="No image provided")
    try:
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        location_response = requests.get("http://localhost/api_php/get_lat_long_location.php")
        if location_response.status_code != 200:
            raise HTTPException(status_code=500, detail="Failed to fetch location data")
        location_data = location_response.json()
        logging.info(f"Fetched location data: {location_data}")
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
        # ‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡∏´‡∏ô‡πà‡∏ß‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏¥‡πÇ‡∏•‡πÄ‡∏°‡∏ï‡∏£
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
        all_locations_in_range = []
        for location in location_data:
            loc_lat = float(location["Location_Lat"])
            loc_lon = float(location["Location_Long"])
            distance = haversine(latitude, longitude, loc_lat, loc_lon)
            if distance <= max_distance:
                all_locations_in_range.append({
                    "location": location["Location_Class"],
                    "location_id": location["Location_Id"],
                    "latitude": loc_lat,
                    "longitude": loc_lon,
                    "distance": round(distance, 2),
                })
        logging.info(f"Locations within range: {all_locations_in_range}")
        # ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏û
        image_bytes = await image.read()
        processed_image = preprocess_image_from_bytes(image_bytes)
        logging.info("Image preprocessing completed.")
        # ‡∏™‡∏Å‡∏±‡∏î‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏à‡∏≤‡∏Å EfficientNet
        features = efficientnet_model.predict(processed_image)
        features = features.flatten()
        logging.info(f"Extracted features shape: {features.shape}")
        # ‡πÉ‡∏ä‡πâ KNN ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ 50 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏ö‡πâ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        distances, indices = knn_model.kneighbors([features], n_neighbors=50)
        logging.info(f"Distances: {distances}")
        logging.info(f"Indices: {indices}")
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        nearest_locations = []
        class_similarity_map = {}
        for index in indices[0]:
            location = train_indices_to_class.get(index, "Unknown")
            similarity = round((1 - distances[0][np.where(indices[0] == index)[0][0]]) * 100, 2)
            if location not in class_similarity_map:
                class_similarity_map[location] = similarity
            else:
                class_similarity_map[location] = max(class_similarity_map[location], similarity)
        sorted_locations = sorted(class_similarity_map.items(), key=lambda x: x[1], reverse=True)
        for location, similarity in sorted_locations:
            if location not in [loc["location"] for loc in nearest_locations]:
                nearest_locations.append({"location": location, "similarity": similarity})
            if len(nearest_locations) == 5:
                break
        logging.info(f"Predicted Nearest Locations: {nearest_locations}")
        # ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏Å‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
        matched_locations = []
        for pred_loc in nearest_locations:
            for range_loc in all_locations_in_range:
                if pred_loc["location"] == range_loc["location"]:
                    matched_locations.append({
                        "location": pred_loc["location"],
                        "similarity": pred_loc["similarity"],
                        "distance": range_loc["distance"],
                        "location_id": range_loc["location_id"],
                    })
        logging.info(f"Matched Locations: {matched_locations}")
        logging.info(f"Locations in Range: {all_locations_in_range}")
        return {
            "matched_locations": matched_locations,
            "all_locations_in_range": all_locations_in_range,
            "nearest_locations": nearest_locations,
        }
    except Exception as e:
        logging.error(f"Error during prediction: {e}")
        raise HTTPException(status_code=500, detail=str(e))
SIMILARITY_THRESHOLD = 50  # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡πÄ‡∏õ‡πá‡∏ô 50%



from tensorflow.keras.preprocessing import image
from sklearn.neighbors import KNeighborsClassifier
import numpy as np
import joblib
import tensorflow as tf
from tensorflow.keras.applications.efficientnet import EfficientNetB7, preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.models import Model
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import shutil
from typing import List
# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•
FEATURES_PATH = "models/features.npy"
LABELS_PATH = "models/labels.npy"
KNN_MODEL_PATH = "models/knn_model.joblib"
NEW_IMAGE_PATH = "C:/xampp/htdocs/api_php/new_image_unverified"


dataset_path = "C:/xampp/htdocs/api_php/new_image_unverified"
save_dir = "models/"
os.makedirs(save_dir, exist_ok=True)
img_height, img_width = 600, 600
k = 5

# Load EfficientNetB7 Model
base_model = EfficientNetB7(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))
x = base_model.output
x = tf.keras.layers.GlobalAveragePooling2D()(x)
feature_extractor = Model(inputs=base_model.input, outputs=x)


def extract_feature(image_path):
    img = load_img(image_path, target_size=(img_height, img_width))
    img_array = img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = preprocess_input(img_array)
    return feature_extractor.predict(img_array)[0]


@app.post("/extract_features/")
async def extract_features():
    features = []
    labels = []
    index_to_class = {}

    for class_name in os.listdir(dataset_path):
        class_dir = os.path.join(dataset_path, class_name)
        if os.path.isdir(class_dir):
            for image_name in os.listdir(class_dir):
                image_path = os.path.join(class_dir, image_name)
                feature = extract_feature(image_path)
                features.append(feature)
                labels.append(class_name)
    
    features = np.array(features)
    np.save(os.path.join(save_dir, 'features.npy'), features)
    np.save(os.path.join(save_dir, 'labels.npy'), np.array(labels))
    return {"message": "Features extracted successfully."}


@app.post("/extract_features_bulk_old/")
async def extract_features_bulk():
    existing_features = np.load(os.path.join(save_dir, 'features.npy'))
    existing_labels = np.load(os.path.join(save_dir, 'labels.npy'))
    
    # ‡πÇ‡∏´‡∏•‡∏î train_indices_to_class ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
    train_indices_to_class_path = os.path.join(save_dir, 'train_indices_to_class.joblib')
    if os.path.exists(train_indices_to_class_path):
        train_indices_to_class = joblib.load(train_indices_to_class_path)
    else:
        train_indices_to_class = {}

    new_features = []
    new_labels = []

    for class_name in os.listdir(dataset_path):
        class_dir = os.path.join(dataset_path, class_name)
        if os.path.isdir(class_dir):
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï index_to_class
            if class_name not in index_to_class.values():
                new_class_id = len(index_to_class)  
                index_to_class[new_class_id] = class_name  

            class_id = list(index_to_class.keys())[list(index_to_class.values()).index(class_name)]

            for image_name in os.listdir(class_dir):
                image_path = os.path.join(class_dir, image_name)
                feature = extract_feature(image_path)
                new_features.append(feature)
                new_labels.append(class_id)

                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï train_indices_to_class
                new_index = len(existing_features) + len(new_features) - 1
                train_indices_to_class[new_index] = class_name

    if new_features:
        updated_features = np.vstack((existing_features, np.array(new_features)))
        updated_labels = np.concatenate((existing_labels, np.array(new_labels)))
    else:
        updated_features, updated_labels = existing_features, existing_labels

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå, labels ‡πÅ‡∏•‡∏∞ index mapping
    np.save(os.path.join(save_dir, 'features.npy'), updated_features)
    np.save(os.path.join(save_dir, 'labels.npy'), updated_labels)
    joblib.dump(index_to_class, os.path.join(save_dir, 'index_to_class.joblib'))
    joblib.dump(train_indices_to_class, TRAIN_INDICES_TO_CLASS)
    

    return {
        "message": "Bulk feature extraction completed.",
        "new_classes": list(index_to_class.values()),
        "train_indices_count": len(train_indices_to_class)
    }
@app.post("/retrain_old/")
async def retrain():
    features = np.load(os.path.join(save_dir, 'features.npy'))
    labels = np.load(os.path.join(save_dir, 'labels.npy'))

    if len(set(labels)) < 2:
        return {"error": "‡∏°‡∏µ‡∏Ñ‡∏•‡∏≤‡∏™‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô"}

    # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    X_train, X_test, y_train, y_test = train_test_split(
        features, labels, test_size=0.2, random_state=42, stratify=labels
    )

    # ‚úÖ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï train_indices_to_class ‡∏ï‡∏≤‡∏° Colab
    train_indices_to_class = {i: index_to_class[y] for i, y in enumerate(y_train.tolist())}

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• KNN
    knn = KNeighborsClassifier(n_neighbors=k, metric='cosine')
    knn.fit(X_train, y_train)

    # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•
    y_pred = knn.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• + index mapping
    joblib.dump(knn, os.path.join(save_dir, 'knn_model.joblib'))
    joblib.dump(train_indices_to_class, os.path.join(save_dir, 'train_indices_to_class.joblib'))
    print('accuracy',accuracy*100)
    return {
        "message": "Model retrained successfully.",
        "accuracy": f"{accuracy * 100:.2f}%",
        "train_indices_count": len(train_indices_to_class)
    }

@app.post("/compare_accuracy/")
async def compare_accuracy():
    try:
        knn_model_path = os.path.join(save_dir, 'knn_model.joblib')

        if not os.path.exists(knn_model_path):
            return {"error": "dont have file model"}

        # üìå ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Å‡πà‡∏≤
        knn_old = joblib.load(knn_model_path)

        # üìå ‡πÇ‡∏´‡∏•‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤
        X_train_old = np.load(os.path.join(save_dir, 'X_train.npy'))
        X_test_old = np.load(os.path.join(save_dir, 'X_test.npy'))
        y_train_old = np.load(os.path.join(save_dir, 'y_train.npy'))
        y_test_old = np.load(os.path.join(save_dir, 'y_test.npy'))

        # üìå ‡πÇ‡∏´‡∏•‡∏î‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        features = np.load(os.path.join(save_dir, 'features.npy'))
        labels = np.load(os.path.join(save_dir, 'labels.npy'))

        # üìå ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
        X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(
            features, labels, test_size=0.2, random_state=42, stratify=labels
        )

        # **‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà**
        if len(X_test_new) == 0 or len(y_test_new) == 0:
            return {"error": "dont have dataset"}

        # üìå ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤ + ‡πÉ‡∏´‡∏°‡πà
        X_train_all = np.concatenate((X_train_old, X_train_new))
        X_test_all = np.concatenate((X_test_old, X_test_new))
        y_train_all = np.concatenate((y_train_old, y_train_new))
        y_test_all = np.concatenate((y_test_old, y_test_new))

        # üìå ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Å‡πà‡∏≤
        old_accuracy_old_test = accuracy_score(y_test_old, knn_old.predict(X_test_old)) * 100
        old_accuracy_new_test = accuracy_score(y_test_new, knn_old.predict(X_test_new)) * 100

        # üìå ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà
        knn_new = KNeighborsClassifier(n_neighbors=5, metric='cosine')
        knn_new.fit(X_train_all, y_train_all)

        # üìå ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà
        new_accuracy_old_test = accuracy_score(y_test_old, knn_new.predict(X_test_old)) * 100
        new_accuracy_new_test = accuracy_score(y_test_new, knn_new.predict(X_test_new)) * 100

        # üìå ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà
        joblib.dump(knn_new, knn_model_path)

        # üìå Debugging
        result = {
            "message": "compare_accuracy successfully.",
            "old_model_accuracy_on_old_test": f"{old_accuracy_old_test:.2f}%",
            "old_model_accuracy_on_new_test": f"{old_accuracy_new_test:.2f}%",
            "new_model_accuracy_on_old_test": f"{new_accuracy_old_test:.2f}%",
            "new_model_accuracy_on_new_test": f"{new_accuracy_new_test:.2f}%",
        }

        print("API Response:", result)  # ‚úÖ Debugging
        return result
    
    except Exception as e:
        print(f"API Error: {str(e)}")  # üî¥ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö error
        return {"error": f"API Error: {str(e)}"}
@app.post("/extract_features_bulk1/")
async def extract_features_bulk():
    existing_features = np.load(os.path.join(save_dir, 'features.npy'))
    existing_labels = np.load(os.path.join(save_dir, 'labels.npy'))

    train_indices_to_class_path = os.path.join(save_dir, 'train_indices_to_class.joblib')
    if os.path.exists(train_indices_to_class_path):
        train_indices_to_class = joblib.load(train_indices_to_class_path)
    else:
        train_indices_to_class = {}

    new_features, new_labels = [], []

    for class_name in os.listdir(dataset_path):
        class_dir = os.path.join(dataset_path, class_name)
        if os.path.isdir(class_dir):
            if class_name not in index_to_class.values():
                new_class_id = len(index_to_class)  
                index_to_class[new_class_id] = class_name  

            class_id = list(index_to_class.keys())[list(index_to_class.values()).index(class_name)]

            for image_name in os.listdir(class_dir):
                image_path = os.path.join(class_dir, image_name)
                feature = extract_feature(image_path)
                new_features.append(feature)
                new_labels.append(class_id)

                new_index = len(existing_features) + len(new_features) - 1
                train_indices_to_class[new_index] = class_name

    if new_features:
        updated_features = np.vstack((existing_features, np.array(new_features)))
        updated_labels = np.concatenate((existing_labels, np.array(new_labels)))
    else:
        updated_features, updated_labels = existing_features, existing_labels

    # **‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train/Test**
    X_train, X_test, y_train, y_test = train_test_split(
        updated_features, updated_labels, test_size=0.2, random_state=42, stratify=updated_labels
    )

    # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÅ‡∏ö‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß
    np.save(os.path.join(save_dir, 'X_train.npy'), X_train)
    np.save(os.path.join(save_dir, 'X_test.npy'), X_test)
    np.save(os.path.join(save_dir, 'y_train.npy'), y_train)
    np.save(os.path.join(save_dir, 'y_test.npy'), y_test)

    # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å index mapping
    np.save(os.path.join(save_dir, 'features.npy'), updated_features)
    np.save(os.path.join(save_dir, 'labels.npy'), updated_labels)
    joblib.dump(index_to_class, os.path.join(save_dir, 'index_to_class.joblib'))
    joblib.dump(train_indices_to_class, train_indices_to_class_path)

    return {
        "message": "Bulk feature extraction completed.",
        "new_classes": list(index_to_class.values()),
        "train_indices_count": len(train_indices_to_class)
    }
@app.post("/retrain1/")
async def retrain():
    features = np.load(os.path.join(save_dir, 'features.npy'))
    labels = np.load(os.path.join(save_dir, 'labels.npy'))
    
    if len(set(labels)) < 2:
        return {"error": "‡∏°‡∏µ‡∏Ñ‡∏•‡∏≤‡∏™‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô"}
    
    # ‡πÇ‡∏´‡∏•‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏™‡πÄ‡∏î‡∏¥‡∏°
    X_test_path = os.path.join(save_dir, 'X_test.npy')
    y_test_path = os.path.join(save_dir, 'y_test.npy')
    
    if os.path.exists(X_test_path) and os.path.exists(y_test_path):
        X_test_old = np.load(X_test_path)
        y_test_old = np.load(y_test_path)
    else:
        X_test_old, y_test_old = None, None
    
    # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
    knn_old_path = os.path.join(save_dir, 'knn_model.joblib')
    if os.path.exists(knn_old_path) and X_test_old is not None:
        knn_old = joblib.load(knn_old_path)
        y_pred_old = knn_old.predict(X_test_old)
        old_accuracy = accuracy_score(y_test_old, y_pred_old)
    else:
        old_accuracy = None
    
    # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
    X_train, X_test_new, y_train, y_test_new = train_test_split(
        features, labels, test_size=0.2, random_state=42, stratify=labels
    )
    
    # ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà
    knn_new = KNeighborsClassifier(n_neighbors=k, metric='cosine')
    knn_new.fit(X_train, y_train)
    y_pred_new = knn_new.predict(X_test_new)
    new_accuracy = accuracy_score(y_test_new, y_pred_new)
    
    # ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    if old_accuracy is None or new_accuracy >= old_accuracy - 0.02:  # ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÉ‡∏´‡πâ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ (2%)
        joblib.dump(knn_new, knn_old_path)
        joblib.dump({i: index_to_class[y] for i, y in enumerate(y_train.tolist())}, 
                    os.path.join(save_dir, 'train_indices_to_class.joblib'))
        
        # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏™‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡πÄ‡∏î‡∏¥‡∏°
        if X_test_old is not None:
            X_test_final = np.vstack((X_test_old, X_test_new))
            y_test_final = np.concatenate((y_test_old, y_test_new))
        else:
            X_test_final, y_test_final = X_test_new, y_test_new
        
        np.save(X_test_path, X_test_final)
        np.save(y_test_path, y_test_final)
        print("Model retrained successfully.")  # ‚úÖ Debugging
        print(f"Old Accuracy: {old_accuracy * 100:.2f}%")
        print(f"New Accuracy: {new_accuracy * 100:.2f}%")
        print("Updated model saved.")  # ‚úÖ Debugging
        return {
            "message": "Model retrained successfully.",
            "old_accuracy": f"{old_accuracy * 100:.2f}%" if old_accuracy else "N/A",
            "new_accuracy": f"{new_accuracy * 100:.2f}%",
            "status": "Updated model saved."
        }
    else:
        print("Model retraining skipped.")  # ‚úÖ Debugging
        print(f"Old Accuracy: {old_accuracy * 100:.2f}%")  # ‚úÖ Debugging
        print(f"New Accuracy: {new_accuracy * 100:.2f}%")  # ‚úÖ Debugging
        print("Old Model retained.")  # ‚úÖ Debugging
        return {
            "message": "Model retraining skipped.",
            "old_accuracy": f"{old_accuracy * 100:.2f}%",
            "new_accuracy": f"{new_accuracy * 100:.2f}%",
            "status": "Old model retained."
        }


@app.post("/extract_features_bulk/")
async def extract_features_bulk():
    existing_features = np.load(os.path.join(save_dir, 'features.npy'))
    existing_labels = np.load(os.path.join(save_dir, 'labels.npy'))

    train_indices_to_class_path = os.path.join(save_dir, 'train_indices_to_class.joblib')
    if os.path.exists(train_indices_to_class_path):
        train_indices_to_class = joblib.load(train_indices_to_class_path)
    else:
        train_indices_to_class = {}

    new_features, new_labels = [], []

    for class_name in os.listdir(dataset_path):
        class_dir = os.path.join(dataset_path, class_name)
        if os.path.isdir(class_dir):
            # ‡πÉ‡∏ä‡πâ index_to_class ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÅ‡∏•‡πâ‡∏ß
            if class_name not in index_to_class.values():
                new_class_id = len(index_to_class)  
                index_to_class[new_class_id] = class_name  

            class_id = list(index_to_class.keys())[list(index_to_class.values()).index(class_name)]

            for image_name in os.listdir(class_dir):
                image_path = os.path.join(class_dir, image_name)
                feature = extract_feature(image_path)
                new_features.append(feature)
                new_labels.append(class_id)

                new_index = len(existing_features) + len(new_features) - 1
                train_indices_to_class[new_index] = class_name

    if new_features:
        updated_features = np.vstack((existing_features, np.array(new_features)))
        updated_labels = np.concatenate((existing_labels, np.array(new_labels)))
    else:
        updated_features, updated_labels = existing_features, existing_labels

    # ‚úÖ ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train/Test
    X_train, X_test, y_train, y_test = train_test_split(
        updated_features, updated_labels, test_size=0.2, random_state=42, stratify=updated_labels
    )

    # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    np.save(os.path.join(save_dir, 'X_train.npy'), X_train)
    np.save(os.path.join(save_dir, 'X_test.npy'), X_test)
    np.save(os.path.join(save_dir, 'y_train.npy'), y_train)
    np.save(os.path.join(save_dir, 'y_test.npy'), y_test)

    np.save(os.path.join(save_dir, 'features.npy'), updated_features)
    np.save(os.path.join(save_dir, 'labels.npy'), updated_labels)
    joblib.dump(index_to_class, INDEX_TO_CLASS_PATH)
    joblib.dump(train_indices_to_class, train_indices_to_class_path)

    return {
        "message": "Bulk feature extraction completed.",
        "new_classes": list(index_to_class.values()),
        "train_indices_count": len(train_indices_to_class)
    }

@app.post("/retrain/")
async def retrain():
    features = np.load(os.path.join(save_dir, 'features.npy'))
    labels = np.load(os.path.join(save_dir, 'labels.npy'))
    
    if len(set(labels)) < 2:
        return {"error": "‡∏°‡∏µ‡∏Ñ‡∏•‡∏≤‡∏™‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô"}
    
    # ‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏™‡πÄ‡∏î‡∏¥‡∏°
    X_test_path = os.path.join(save_dir, 'X_test.npy')
    y_test_path = os.path.join(save_dir, 'y_test.npy')
    
    if os.path.exists(X_test_path) and os.path.exists(y_test_path):
        X_test_old = np.load(X_test_path)
        y_test_old = np.load(y_test_path)
    else:
        X_test_old, y_test_old = None, None
    
    # ‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
    knn_old_path = os.path.join(save_dir, 'knn_model.joblib')
    if os.path.exists(knn_old_path) and X_test_old is not None:
        knn_old = joblib.load(knn_old_path)
        y_pred_old = knn_old.predict(X_test_old)
        old_accuracy = accuracy_score(y_test_old, y_pred_old)
    else:
        old_accuracy = None
    
    # ‚úÖ ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
    X_train, X_test_new, y_train, y_test_new = train_test_split(
        features, labels, test_size=0.2, random_state=42, stratify=labels
    )
    
    # ‚úÖ ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà
    knn_new = KNeighborsClassifier(n_neighbors=k, metric='cosine')
    knn_new.fit(X_train, y_train)
    y_pred_new = knn_new.predict(X_test_new)
    new_accuracy = accuracy_score(y_test_new, y_pred_new)
    
    # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á
    accuracy_log_path = os.path.join(save_dir, 'accuracy_history.txt')
    with open(accuracy_log_path, 'a') as f:
        f.write(f"Old: {old_accuracy * 100 if old_accuracy else 'N/A'}% | New: {new_accuracy * 100:.2f}%\n")

    # ‚úÖ ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    if old_accuracy is None or new_accuracy >= old_accuracy - 0.02:  
        joblib.dump(knn_new, knn_old_path)
        joblib.dump({i: index_to_class[y] for i, y in enumerate(y_train.tolist())}, 
                    os.path.join(save_dir, 'train_indices_to_class.joblib'))
        
        if X_test_old is not None:
            X_test_final = np.vstack((X_test_old, X_test_new))
            y_test_final = np.concatenate((y_test_old, y_test_new))
        else:
            X_test_final, y_test_final = X_test_new, y_test_new
        
        np.save(X_test_path, X_test_final)
        np.save(y_test_path, y_test_final)
        print("Model retrained successfully.")  # ‚úÖ Debugging
        print(f"Old Accuracy: {old_accuracy * 100:.2f}%")
        print(f"New Accuracy: {new_accuracy * 100:.2f}%")
        print("Updated model saved.")
        return {
            "message": "Model retrained successfully.",
            "old_accuracy": f"{old_accuracy * 100:.2f}%" if old_accuracy else "N/A",
            "new_accuracy": f"{new_accuracy * 100:.2f}%",
            "status": "Updated model saved."
        }
    else:
        print("Model retraining skipped.")  # ‚úÖ Debugging
        print(f"Old Accuracy: {old_accuracy * 100:.2f}%")  # ‚úÖ Debugging   
        print(f"New Accuracy: {new_accuracy * 100:.2f}%")
        print("Old Model retained.")
        return {
            "message": "Model retraining skipped.",
            "old_accuracy": f"{old_accuracy * 100:.2f}%",
            "new_accuracy": f"{new_accuracy * 100:.2f}%",
            "status": "Old model retained."
        }